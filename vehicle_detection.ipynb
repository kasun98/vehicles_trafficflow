{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from ultralytics import YOLO\n",
    "from ultralytics import YOLO, solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a model\n",
    "model = YOLO(\"yolov8n.yaml\")\n",
    "model = YOLO(\"runs/detect/train/weights/best.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accepts all formats - image/dir/Path/URL/video/PIL/ndarray. 0 for webcam\n",
    "\n",
    "#results = model.predict(source=\"0\", show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '3w', 1: 'bike', 2: 'bus', 3: 'car', 4: 'lorry', 5: 'person', 6: 'van'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your YOLO model\n",
    "model = model\n",
    "\n",
    "# 0 for webcam, 'filepath' for video files\n",
    "cap = cv2.VideoCapture(\"vids/vid1.mp4\")\n",
    "assert cap.isOpened(), \"Error reading video file\"\n",
    "w, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\n",
    "\n",
    "out = cv2.VideoWriter(\"pie_chart.avi\", cv2.VideoWriter_fourcc(*\"MJPG\"), fps, (w, h))\n",
    "\n",
    "analytics = solutions.Analytics(\n",
    "    type=\"bar\",\n",
    "    writer=out,\n",
    "    im0_shape=(w, h),\n",
    "    view_img=True,\n",
    ")\n",
    "\n",
    "# Initialize the cumulative class count dictionary\n",
    "clswise_count = {}\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if success:\n",
    "        results = model.track(frame, persist=True, verbose=True)\n",
    "        if results[0].boxes.id is not None:\n",
    "            boxes = results[0].boxes.xyxy.cpu()\n",
    "            clss = results[0].boxes.cls.cpu().tolist()\n",
    "            for box, cls in zip(boxes, clss):\n",
    "                class_name = model.names[int(cls)]\n",
    "                if class_name in clswise_count:\n",
    "                    clswise_count[class_name] += 1\n",
    "                else:\n",
    "                    clswise_count[class_name] = 1\n",
    "\n",
    "            analytics.update_bar(clswise_count)\n",
    "        \n",
    "        # Display the frame with the real-time pie chart (optional)\n",
    "        results = model.predict(source=frame, show=True)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clswise_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line Counter Initiated.\n",
      "\n",
      "0: 512x736 2 cars, 800.4ms\n",
      "Speed: 18.5ms preprocess, 800.4ms inference, 2.4ms postprocess per image at shape (1, 3, 512, 736)\n",
      "\n",
      "0: 512x736 1 3w, 1 bike, 3 cars, 948.0ms\n",
      "Speed: 16.7ms preprocess, 948.0ms inference, 3.0ms postprocess per image at shape (1, 3, 512, 736)\n",
      "\n",
      "0: 512x736 1 3w, 1 bike, 3 cars, 816.1ms\n",
      "Speed: 16.0ms preprocess, 816.1ms inference, 1.0ms postprocess per image at shape (1, 3, 512, 736)\n",
      "\n",
      "0: 512x736 1 3w, 1 bike, 3 cars, 769.4ms\n",
      "Speed: 9.2ms preprocess, 769.4ms inference, 1.0ms postprocess per image at shape (1, 3, 512, 736)\n",
      "\n",
      "0: 512x736 1 3w, 1 bike, 3 cars, 802.1ms\n",
      "Speed: 13.1ms preprocess, 802.1ms inference, 1.0ms postprocess per image at shape (1, 3, 512, 736)\n",
      "\n",
      "0: 512x736 1 3w, 1 bike, 3 cars, 742.0ms\n",
      "Speed: 9.0ms preprocess, 742.0ms inference, 1.0ms postprocess per image at shape (1, 3, 512, 736)\n",
      "\n",
      "0: 512x736 1 3w, 1 bike, 3 cars, 674.3ms\n",
      "Speed: 14.0ms preprocess, 674.3ms inference, 1.0ms postprocess per image at shape (1, 3, 512, 736)\n",
      "\n",
      "0: 512x736 1 3w, 1 bike, 3 cars, 669.4ms\n",
      "Speed: 13.2ms preprocess, 669.4ms inference, 1.0ms postprocess per image at shape (1, 3, 512, 736)\n",
      "\n",
      "0: 512x736 1 3w, 1 bike, 3 cars, 657.6ms\n",
      "Speed: 17.0ms preprocess, 657.6ms inference, 2.0ms postprocess per image at shape (1, 3, 512, 736)\n",
      "\n",
      "0: 512x736 1 3w, 1 bike, 3 cars, 677.4ms\n",
      "Speed: 13.0ms preprocess, 677.4ms inference, 5.0ms postprocess per image at shape (1, 3, 512, 736)\n",
      "\n",
      "0: 512x736 1 3w, 1 bike, 3 cars, 690.1ms\n",
      "Speed: 16.5ms preprocess, 690.1ms inference, 2.0ms postprocess per image at shape (1, 3, 512, 736)\n",
      "\n",
      "0: 512x736 1 3w, 1 bike, 3 cars, 727.3ms\n",
      "Speed: 6.0ms preprocess, 727.3ms inference, 1.0ms postprocess per image at shape (1, 3, 512, 736)\n",
      "\n",
      "0: 512x736 1 3w, 1 bike, 3 cars, 704.6ms\n",
      "Speed: 6.0ms preprocess, 704.6ms inference, 1.0ms postprocess per image at shape (1, 3, 512, 736)\n",
      "\n",
      "0: 512x736 1 3w, 1 bike, 3 cars, 699.5ms\n",
      "Speed: 15.4ms preprocess, 699.5ms inference, 3.0ms postprocess per image at shape (1, 3, 512, 736)\n",
      "\n",
      "0: 512x736 1 3w, 1 bike, 3 cars, 1099.2ms\n",
      "Speed: 6.0ms preprocess, 1099.2ms inference, 1.0ms postprocess per image at shape (1, 3, 512, 736)\n",
      "\n",
      "0: 512x736 1 3w, 1 bike, 3 cars, 506.3ms\n",
      "Speed: 0.0ms preprocess, 506.3ms inference, 0.0ms postprocess per image at shape (1, 3, 512, 736)\n",
      "\n",
      "0: 512x736 1 3w, 1 bike, 3 cars, 561.0ms\n",
      "Speed: 10.6ms preprocess, 561.0ms inference, 0.0ms postprocess per image at shape (1, 3, 512, 736)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 88\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     87\u001b[0m frame_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 88\u001b[0m tracks \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpersist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclasses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclasses_to_count\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m im0 \u001b[38;5;241m=\u001b[39m counter\u001b[38;5;241m.\u001b[39mstart_counting(im0, tracks)\n\u001b[0;32m     90\u001b[0m im0 \u001b[38;5;241m=\u001b[39m speed_obj\u001b[38;5;241m.\u001b[39mestimate_speed(im0, tracks)\n",
      "File \u001b[1;32mc:\\Users\\kdkas\\Desktop\\projects\\yoloemotion\\venv\\lib\\site-packages\\ultralytics\\engine\\model.py:493\u001b[0m, in \u001b[0;36mModel.track\u001b[1;34m(self, source, stream, persist, **kwargs)\u001b[0m\n\u001b[0;32m    491\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# batch-size 1 for tracking in videos\u001b[39;00m\n\u001b[0;32m    492\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrack\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 493\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(source\u001b[38;5;241m=\u001b[39msource, stream\u001b[38;5;241m=\u001b[39mstream, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kdkas\\Desktop\\projects\\yoloemotion\\venv\\lib\\site-packages\\ultralytics\\engine\\model.py:453\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_prompts\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[0;32m    452\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mset_prompts(prompts)\n\u001b[1;32m--> 453\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_cli(source\u001b[38;5;241m=\u001b[39msource) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kdkas\\Desktop\\projects\\yoloemotion\\venv\\lib\\site-packages\\ultralytics\\engine\\predictor.py:168\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[1;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kdkas\\Desktop\\projects\\yoloemotion\\venv\\lib\\site-packages\\torch\\utils\\_contextlib.py:35\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m---> 35\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     38\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     39\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kdkas\\Desktop\\projects\\yoloemotion\\venv\\lib\\site-packages\\ultralytics\\engine\\predictor.py:248\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[1;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m--> 248\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minference(im, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39membed:\n\u001b[0;32m    250\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m [preds] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(preds, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m preds  \u001b[38;5;66;03m# yield embedding tensors\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kdkas\\Desktop\\projects\\yoloemotion\\venv\\lib\\site-packages\\ultralytics\\engine\\predictor.py:142\u001b[0m, in \u001b[0;36mBasePredictor.inference\u001b[1;34m(self, im, *args, **kwargs)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Runs inference on a given image using the specified model and arguments.\"\"\"\u001b[39;00m\n\u001b[0;32m    137\u001b[0m visualize \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    138\u001b[0m     increment_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_dir \u001b[38;5;241m/\u001b[39m Path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mstem, mkdir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mvisualize \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_type\u001b[38;5;241m.\u001b[39mtensor)\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    141\u001b[0m )\n\u001b[1;32m--> 142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(im, augment\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39maugment, visualize\u001b[38;5;241m=\u001b[39mvisualize, embed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39membed, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kdkas\\Desktop\\projects\\yoloemotion\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kdkas\\Desktop\\projects\\yoloemotion\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kdkas\\Desktop\\projects\\yoloemotion\\venv\\lib\\site-packages\\ultralytics\\nn\\autobackend.py:453\u001b[0m, in \u001b[0;36mAutoBackend.forward\u001b[1;34m(self, im, augment, visualize, embed)\u001b[0m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;66;03m# PyTorch\u001b[39;00m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpt \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_module:\n\u001b[1;32m--> 453\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(im, augment\u001b[38;5;241m=\u001b[39maugment, visualize\u001b[38;5;241m=\u001b[39mvisualize, embed\u001b[38;5;241m=\u001b[39membed)\n\u001b[0;32m    455\u001b[0m \u001b[38;5;66;03m# TorchScript\u001b[39;00m\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjit:\n",
      "File \u001b[1;32mc:\\Users\\kdkas\\Desktop\\projects\\yoloemotion\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kdkas\\Desktop\\projects\\yoloemotion\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kdkas\\Desktop\\projects\\yoloemotion\\venv\\lib\\site-packages\\ultralytics\\nn\\tasks.py:89\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[1;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kdkas\\Desktop\\projects\\yoloemotion\\venv\\lib\\site-packages\\ultralytics\\nn\\tasks.py:107\u001b[0m, in \u001b[0;36mBaseModel.predict\u001b[1;34m(self, x, profile, visualize, augment, embed)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_augment(x)\n\u001b[1;32m--> 107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kdkas\\Desktop\\projects\\yoloemotion\\venv\\lib\\site-packages\\ultralytics\\nn\\tasks.py:128\u001b[0m, in \u001b[0;36mBaseModel._predict_once\u001b[1;34m(self, x, profile, visualize, embed)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[1;32m--> 128\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[0;32m    129\u001b[0m y\u001b[38;5;241m.\u001b[39mappend(x \u001b[38;5;28;01mif\u001b[39;00m m\u001b[38;5;241m.\u001b[39mi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
      "File \u001b[1;32mc:\\Users\\kdkas\\Desktop\\projects\\yoloemotion\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kdkas\\Desktop\\projects\\yoloemotion\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kdkas\\Desktop\\projects\\yoloemotion\\venv\\lib\\site-packages\\ultralytics\\nn\\modules\\block.py:229\u001b[0m, in \u001b[0;36mC2f.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m    228\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass through C2f layer.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 229\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    230\u001b[0m     y\u001b[38;5;241m.\u001b[39mextend(m(y[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mm)\n\u001b[0;32m    231\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv2(torch\u001b[38;5;241m.\u001b[39mcat(y, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\kdkas\\Desktop\\projects\\yoloemotion\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kdkas\\Desktop\\projects\\yoloemotion\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kdkas\\Desktop\\projects\\yoloemotion\\venv\\lib\\site-packages\\ultralytics\\nn\\modules\\conv.py:54\u001b[0m, in \u001b[0;36mConv.forward_fuse\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_fuse\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     53\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Perform transposed convolution of 2D data.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\kdkas\\Desktop\\projects\\yoloemotion\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kdkas\\Desktop\\projects\\yoloemotion\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kdkas\\Desktop\\projects\\yoloemotion\\venv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kdkas\\Desktop\\projects\\yoloemotion\\venv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 900x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "cap = cv2.VideoCapture(\"vids/vid1.mp4\")\n",
    "assert cap.isOpened(), \"Error reading video file\"\n",
    "w, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\n",
    "\n",
    "\n",
    "# Video writer\n",
    "video_writer = cv2.VideoWriter(\"speed_estimation.avi\", cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w, h))\n",
    "out = cv2.VideoWriter(\"bar_chart.avi\", cv2.VideoWriter_fourcc(*\"MJPG\"), fps, (900, h))\n",
    "out2 = cv2.VideoWriter(\"multiple_line_plot.avi\", cv2.VideoWriter_fourcc(*\"MJPG\"), fps, (w, h))\n",
    "\n",
    "line_pts = [(0, 100), (720, 250)]\n",
    "line_pts_spd_out = [(0, 100), (340, 170)]\n",
    "line_pts_spd_in = [(340, 170), (720, 250)]\n",
    "classes_to_count = [0, 1, 2, 3, 4, 6]\n",
    "\n",
    "# Init speed-estimation obj\n",
    "speed_obj = solutions.SpeedEstimator(\n",
    "    reg_pts=line_pts_spd_out,\n",
    "    names=model.names,\n",
    "    view_img=True,\n",
    "    line_thickness=1,\n",
    "    region_thickness=2,\n",
    "    spdl_dist_thresh=20,\n",
    ")\n",
    "\n",
    "speed_obj2 = solutions.SpeedEstimator(\n",
    "    reg_pts=line_pts_spd_in,\n",
    "    names=model.names,\n",
    "    view_img=True,\n",
    "    line_thickness=1,\n",
    "    region_thickness=2,\n",
    "    spdl_dist_thresh=20,\n",
    ")\n",
    "\n",
    "# Init Object Counter\n",
    "counter = solutions.ObjectCounter(\n",
    "    view_img=False,\n",
    "    reg_pts=line_pts,\n",
    "    classes_names=model.names,\n",
    "    draw_tracks=False,\n",
    "    line_thickness=1,\n",
    "    track_thickness=1,\n",
    "    region_thickness=2,\n",
    ")\n",
    "\n",
    "analytics = solutions.Analytics(\n",
    "    type=\"bar\",\n",
    "    writer=out,\n",
    "    title = \"Classwise Total Vehicle Flow\",\n",
    "    im0_shape=(900, h),\n",
    "    view_img=True,\n",
    ")\n",
    "\n",
    "analytics2 = solutions.Analytics(\n",
    "    type=\"line\",\n",
    "    writer=out2,\n",
    "    line_width=1,\n",
    "    im0_shape=(w, h),\n",
    "    view_img=True,\n",
    "    max_points=200,\n",
    "    \n",
    ")\n",
    "\n",
    "def transform_class_wise_count(counter):\n",
    "    result = {}\n",
    "    for vehicle, counts in counter.items():\n",
    "        for direction, count in counts.items():\n",
    "            result[f\"{vehicle}_{direction}\"] = count\n",
    "    return result\n",
    "\n",
    "def aggregate_counts(counter):\n",
    "    result = {'IN': 0, 'OUT': 0}\n",
    "    for vehicle, counts in counter.items():\n",
    "        for direction, count in counts.items():\n",
    "            result[direction] += count\n",
    "    return result\n",
    "\n",
    "frame_count = 0\n",
    "data = {}\n",
    "labels = ['IN','OUT']\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, im0 = cap.read()\n",
    "    if not success:\n",
    "        print(\"Video frame is empty or video processing has been successfully completed.\")\n",
    "        break\n",
    "    frame_count += 1\n",
    "    tracks = model.track(im0, persist=True, show=False, classes=classes_to_count)\n",
    "    im0 = counter.start_counting(im0, tracks)\n",
    "    im0 = speed_obj.estimate_speed(im0, tracks)\n",
    "    im0 = speed_obj2.estimate_speed(im0, tracks)\n",
    "    if counter.class_wise_count is not None:\n",
    "        \n",
    "        analytics.update_bar(transform_class_wise_count(counter.class_wise_count))\n",
    "        data = aggregate_counts(counter.class_wise_count)\n",
    "        \n",
    "\n",
    "    analytics2.update_multiple_lines(data, labels, frame_count)\n",
    "    #data = {}\n",
    "    video_writer.write(im0)\n",
    "\n",
    "cap.release()\n",
    "video_writer.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(np.mean(list(speed_obj.dist_data.values())),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(speed_obj.dist_data.values()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.1798085945049177"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(np.mean(list(speed_obj2.dist_data.values())),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_spead(obj):\n",
    "\n",
    "\n",
    "    dist_data_values = list(obj.dist_data.values())\n",
    "\n",
    "    # Check if the list is empty\n",
    "    if dist_data_values:\n",
    "        mean_value = np.mean(dist_data_values)\n",
    "    else:\n",
    "        mean_value = 0\n",
    "\n",
    "    return mean_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.1798085945049177"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_spead(speed_obj2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['IN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 1, 0, 0, 0]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(transform_class_wise_count(counter.class_wise_count).values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3w', 'car', 'bike']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(counter.class_wise_count.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'IN': 1, 'OUT': 0}, {'IN': 1, 'OUT': 0}, {'IN': 0, 'OUT': 0}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(counter.class_wise_count.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'3w': {'IN': 1, 'OUT': 1},\n",
       " 'car': {'IN': 1, 'OUT': 0},\n",
       " 'bike': {'IN': 0, 'OUT': 0}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter.class_wise_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
